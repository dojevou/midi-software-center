# Phase 5: Commands Layer Testing (Comprehensive Plan)

**Objective:** Test all Tauri IPC commands across Pipeline (11 files) and DAW (7 files) applications
**Coverage Target:** 80%+ on all command functions
**Estimated Duration:** 8-12 hours (tool-stacked workflow)
**Expected Outcome:** ~200-250 tests covering all commands

## Phase 5 Structure (6 Subphases)

### Phase 5.0: Commands Architecture Analysis âœ… (THIS TASK)
- **Goal:** Map all commands, identify testing patterns, design test architecture
- **Deliverable:** This comprehensive plan document
- **Key Files:**
  - Pipeline: 11 command files (1,200+ lines)
  - DAW: 7 command files (900+ lines)

---

## Phase 5.1: Pipeline Commands - Discovery & Architecture

**Duration:** 60-90 minutes
**Tools:** Explore agent, repo-research-analyst agent, architecture-reviewer agent
**Slash Commands:** `/feature-dev:feature-dev`

### Subphase 5.1.0: Analyze Pipeline Command Structure
```bash
STEP 1: Use Explore agent (very thorough) to map all Pipeline commands
- Task: "Map all Tauri #[tauri::command] functions in pipeline/src-tauri/src/commands/"
- Output: Complete list of:
  * Function signatures
  * Parameters and return types
  * Error handling patterns
  * Database access patterns
  * File I/O operations
- This informs test architecture decisions
```

**Tool Stack:**
```
Task(subagent_type="Explore", thoroughness="very thorough")
  â†“ discovers command patterns
  â†“ identifies database access
  â†“ finds file operations
  â†’ FEEDS INTO â†’ architecture-reviewer agent
```

### Subphase 5.1.1: Design Test Architecture
```bash
STEP 2: Use architecture-reviewer agent to validate test design
- Task: "Design test architecture for 11 Pipeline command files"
- Considerations:
  * Mock vs real database (recommendation: test DB with fixtures)
  * File system mocking (recommendation: temp directories)
  * Progress callback testing
  * Error scenario coverage
  * Batch operation testing (file import, analyze)
```

**Tool Stack:**
```
architecture-reviewer agent
  â†“ analyzes component boundaries
  â†“ validates separation of concerns
  â†“ recommends fixture patterns
  â†’ FEEDS INTO â†’ repository layer structure
```

### Subphase 5.1.2: Map Command Dependencies
```bash
STEP 3: Identify which commands use which repositories
- Grep pattern: Search for repository calls in command files
- Output format: Command â†’ Repository mapping
  * file_import â†’ file_repository, metadata_repository, tag_repository
  * search â†’ search_repository
  * tags â†’ tag_repository
  * analyze â†’ metadata_repository
  * etc.
```

**Tool Stack:**
```
Grep tool (pattern: "Repository::")
  â†“ finds all repository calls
  â†“ maps command dependencies
  â†’ FEEDS INTO â†’ test generation strategy
```

### Subphase 5.1.3: Create Test File Structure
```bash
STEP 4: Create test module stubs for each Pipeline command
Files to create:
  âœ“ pipeline/src-tauri/tests/commands/file_import_test.rs
  âœ“ pipeline/src-tauri/tests/commands/archive_import_test.rs
  âœ“ pipeline/src-tauri/tests/commands/analyze_test.rs
  âœ“ pipeline/src-tauri/tests/commands/search_test.rs
  âœ“ pipeline/src-tauri/tests/commands/tags_test.rs
  âœ“ pipeline/src-tauri/tests/commands/split_file_test.rs
  âœ“ pipeline/src-tauri/tests/commands/files_test.rs
  âœ“ pipeline/src-tauri/tests/commands/stats_test.rs
  âœ“ pipeline/src-tauri/tests/commands/progress_test.rs
  âœ“ pipeline/src-tauri/tests/commands/system_test.rs

(11 test files total)
```

**Tool Stack:**
```
Write tool (bulk operation)
  â†“ creates test module stubs
  â†“ includes setup/teardown boilerplate
  â†’ READY FOR â†’ test generation phase
```

---

## Phase 5.2: Pipeline Commands - Test Suite Generation

**Duration:** 3-4 hours
**Tools:** unit-test-generator plugin, database-test-manager plugin, code-reviewer agent
**Slash Commands:** `/unit-test-generator:generate-tests`, `/database-test-manager:db-test`

### Subphase 5.2.1: Generate Test Cases for Batch Operations
```bash
PRIORITY: file_import, archive_import, analyze (bulk operations)
REASON: These are most complex and most used

STEP 1: Use unit-test-generator for file_import.rs
Task: "Generate comprehensive tests for file_import command"
Expected: ~25-30 tests covering:
  âœ“ Single file import (success, duplicate detection, hash validation)
  âœ“ Batch import (10, 100, 1000 files)
  âœ“ Error scenarios (corrupted files, disk full, DB constraint violations)
  âœ“ Progress callbacks (async notification tracking)
  âœ“ Metadata extraction (format detection, duration calculation)
  âœ“ Concurrent imports (thread safety)
  âœ“ Cancellation (graceful abort mid-import)
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ analyzes function signatures
  â†“ generates test cases
  â†“ includes property-based tests
  â†’ FEEDS INTO â†’ database-test-manager

database-test-manager (for DB isolation)
  â†“ setup test fixtures
  â†“ run tests with transaction rollback
  â†“ verify constraints
  â†’ PRODUCES â†’ 25-30 passing tests
```

### Subphase 5.2.2: Generate Tests for Archive Operations
```bash
Command: archive_import.rs (ZIP, RAR, 7z extraction)

STEP 2: Use unit-test-generator for archive operations
Expected: ~12-15 tests covering:
  âœ“ ZIP extraction (standard, nested, large files)
  âœ“ RAR extraction (version 4, 5 compatibility)
  âœ“ 7z extraction (solid archives, multi-volume)
  âœ“ Error handling (corrupted archives, password-protected)
  âœ“ Path traversal prevention (security validation)
  âœ“ Symlink handling (symlink bombs prevention)
  âœ“ File permission preservation (Unix permissions)
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ generates extraction tests
  â†“ includes malformed archive handling
  â†“ security-focused test cases
  â†’ FEEDS INTO â†’ security-sentinel agent

security-sentinel agent
  â†“ reviews for path traversal
  â†“ validates symlink handling
  â†“ checks extraction safety
  â†’ APPROVES â†’ archive safety baseline
```

### Subphase 5.2.3: Generate Tests for Query Commands
```bash
Commands: search.rs, tags.rs, stats.rs, analysis.rs

STEP 3: Use unit-test-generator for query commands
Expected: ~40-50 tests total covering:
  âœ“ search.rs (12 tests):
    - Full-text search with various filters
    - Pagination and ordering
    - Tag-based filtering
  âœ“ tags.rs (10 tests):
    - Tag creation, deletion, updates
    - Bulk tag operations
    - Tag hierarchy/relationships
  âœ“ stats.rs (8 tests):
    - Aggregate statistics calculations
    - Time-series data (by date, manufacturer)
    - Distribution analysis
  âœ“ files.rs (10 tests):
    - File listing and filtering
    - Metadata retrieval
    - Batch operations
  âœ“ system.rs (6 tests):
    - System status checks
    - Configuration operations
    - Health checks
```

**Tool Stack:**
```
/unit-test-generator:generate-tests (run 5x for different commands)
  â†“ generates query-specific tests
  â†“ includes edge cases (empty results, large datasets)
  â†“ null handling, sorting, pagination
  â†’ FEEDS INTO â†’ query-performance-analyzer

query-performance-analyzer skill
  â†“ analyzes query patterns
  â†“ suggests index optimization
  â†“ validates execution plans
  â†’ PRODUCES â†’ performance baseline
```

### Subphase 5.2.4: Generate Tests for Utility Commands
```bash
Commands: progress.rs, split_file.rs, analyze.rs

STEP 4: Use unit-test-generator for utilities
Expected: ~20-25 tests covering:
  âœ“ progress.rs (8 tests):
    - Progress tracking state machine
    - Concurrent progress updates
    - Callback execution
  âœ“ split_file.rs (10 tests):
    - Single-track to multi-track splitting
    - Track extraction and validation
    - Error recovery (partial splits)
  âœ“ analyze.rs (5-7 tests):
    - MIDI analysis (BPM, key, instruments)
    - Metadata generation
    - Concurrent analysis
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ generates utility tests
  â†“ includes state machine validation
  â†“ concurrency scenarios
  â†’ FEEDS INTO â†’ code-reviewer

code-reviewer agent
  â†“ validates test quality
  â†“ checks coverage completeness
  â†“ identifies missing scenarios
  â†’ PRODUCES â†’ high-quality test suite
```

### Subphase 5.2.5: Code Review & Quality Pass
```bash
STEP 5: Use code-reviewer agent on all Pipeline command tests
Task: "Review all 11 Pipeline command test files for quality"
Focus:
  âœ“ Test isolation (no cross-test dependencies)
  âœ“ Error scenario coverage (all Result<T> variants)
  âœ“ Database transaction safety
  âœ“ Resource cleanup (files, connections)
  âœ“ Assertion quality (not just "assert!(result.is_ok())")
```

**Tool Stack:**
```
/code-reviewer:code-review
  â†“ runs on all 11 test files
  â†“ checks for anti-patterns
  â†“ validates assertion quality
  â†’ PRODUCES â†’ high-quality baseline

Then loop:
If issues found:
  â†“ Use code-simplifier agent
  â†“ Fix identified issues
  â†“ Re-run review
Until: All tests pass quality gate
```

---

## Phase 5.3: DAW Commands - Discovery & Architecture

**Duration:** 45-60 minutes
**Tools:** Explore agent, architecture-reviewer agent, midi-hardware agent
**Slash Commands:** `/feature-dev:feature-dev`

### Subphase 5.3.0: Analyze DAW Command Structure
```bash
STEP 1: Use Explore agent (very thorough) for DAW commands
Task: "Map all Tauri #[tauri::command] functions in daw/src-tauri/src/commands/"
Focus Areas:
  âœ“ Real-time sequencer commands
  âœ“ MIDI hardware integration (midir)
  âœ“ Playback control
  âœ“ Project management
  âœ“ Audio I/O
  âœ“ Analysis/export operations

Output: Complete command inventory with:
  * Function signatures
  * State management patterns
  * MIDI hardware dependencies
  * Playback timing requirements
  * File I/O operations
```

**Tool Stack:**
```
Task(subagent_type="Explore", thoroughness="very thorough")
  â†“ discovers DAW command patterns
  â†“ identifies real-time constraints
  â†“ finds state management approach
  â†’ FEEDS INTO â†’ midi-hardware agent
```

### Subphase 5.3.1: Design DAW Test Architecture
```bash
STEP 2: Use midi-hardware agent to design MIDI testing strategy
Task: "Design test architecture for DAW commands with MIDI hardware simulation"
Considerations:
  âœ“ MIDI hardware mocking (no real device needed)
  âœ“ Real-time timing requirements (sub-ms accuracy)
  âœ“ Playback state machine (play, pause, stop, seek)
  âœ“ Event sequencing and scheduling
  âœ“ MIDI port enumeration
```

**Tool Stack:**
```
midi-hardware agent
  â†“ understands MIDI timing requirements
  â†“ knows hardware simulation patterns
  â†“ reviews MIDI command implementations
  â†’ FEEDS INTO â†’ architecture-reviewer

architecture-reviewer agent
  â†“ validates component isolation
  â†“ ensures testability without hardware
  â†“ recommends fixture design
  â†’ PRODUCES â†’ DAW test architecture
```

### Subphase 5.3.2: Create DAW Test File Structure
```bash
STEP 3: Create test module stubs for each DAW command
Files to create:
  âœ“ daw/src-tauri/tests/commands/sequencer_test.rs
  âœ“ daw/src-tauri/tests/commands/midi_test.rs
  âœ“ daw/src-tauri/tests/commands/project_test.rs
  âœ“ daw/src-tauri/tests/commands/playback_test.rs
  âœ“ daw/src-tauri/tests/commands/export_test.rs
  âœ“ daw/src-tauri/tests/commands/analysis_test.rs
  âœ“ daw/src-tauri/tests/commands/search_test.rs

(7 test files total)
```

**Tool Stack:**
```
Write tool
  â†“ creates test stubs with real-time patterns
  â†“ includes MIDI mock infrastructure
  â†“ adds timing assertion helpers
  â†’ READY FOR â†’ test generation
```

---

## Phase 5.4: DAW Commands - Test Suite Generation

**Duration:** 3-4 hours
**Tools:** unit-test-generator plugin, midi-hardware agent, performance-oracle agent
**Slash Commands:** `/unit-test-generator:generate-tests`, `/database-index-advisor:index-advisor`

### Subphase 5.4.1: Generate Tests for Sequencer Commands
```bash
PRIORITY: sequencer.rs (core DAW functionality)
REASON: Most complex, real-time constraints

STEP 1: Use unit-test-generator for sequencer
Task: "Generate comprehensive tests for sequencer command"
Expected: ~35-40 tests covering:
  âœ“ Sequencer state (play, pause, stop, seek)
  âœ“ Track management (add, remove, reorder)
  âœ“ Event scheduling (note on/off, CC, pitch bend)
  âœ“ Timing accuracy (tempo changes, time signature)
  âœ“ MIDI output (event transmission order)
  âœ“ Concurrent operations (multiple commands while playing)
  âœ“ Edge cases (seek beyond bounds, 0 tempo, invalid track)
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ generates sequencer state machine tests
  â†“ includes timing validation
  â†“ covers edge cases
  â†’ FEEDS INTO â†’ midi-hardware agent

midi-hardware agent
  â†“ validates MIDI event output correctness
  â†“ checks timing accuracy
  â†“ reviews playback logic
  â†’ FEEDS INTO â†’ performance-oracle

performance-oracle agent
  â†“ analyzes real-time performance
  â†“ checks latency requirements (<5ms)
  â†“ validates CPU efficiency
  â†’ PRODUCES â†’ 35-40 production-ready tests
```

### Subphase 5.4.2: Generate Tests for MIDI Hardware Commands
```bash
Commands: midi.rs (hardware I/O)

STEP 2: Use unit-test-generator for MIDI operations
Expected: ~20-25 tests covering:
  âœ“ MIDI port enumeration (list available ports)
  âœ“ MIDI input (hardware â†’ sequencer)
  âœ“ MIDI output (sequencer â†’ hardware)
  âœ“ Hardware disconnection (graceful error handling)
  âœ“ Port permission errors (access denied)
  âœ“ Concurrent I/O (simultaneous input/output)
  âœ“ Data buffering (overflow prevention)
  âœ“ Device reconnection (hot-plug handling)
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ generates MIDI I/O tests
  â†“ includes mock device implementations
  â†“ covers error scenarios
  â†’ FEEDS INTO â†’ security-sentinel

security-sentinel agent
  â†“ validates no buffer overflows
  â†“ checks error handling safety
  â†“ reviews device access safety
  â†’ PRODUCES â†’ 20-25 hardened tests
```

### Subphase 5.4.3: Generate Tests for Project & Export Commands
```bash
Commands: project.rs (project management), export.rs (MIDI export)

STEP 3: Use unit-test-generator for project operations
Expected: ~25-30 tests covering:
  âœ“ project.rs (15 tests):
    - Create/open/save projects
    - Undo/redo state management
    - File versioning
    - Autosave functionality
  âœ“ export.rs (10-15 tests):
    - Export to MIDI file
    - Export to audio (if supported)
    - Format validation
    - Metadata preservation
    - Batch export operations
```

**Tool Stack:**
```
/unit-test-generator:generate-tests (2x)
  â†“ generates project management tests
  â†“ includes file I/O validation
  â†“ covers undo/redo state machine
  â†’ FEEDS INTO â†’ database-test-manager

/database-test-manager:db-test
  â†“ if projects use database
  â†“ validates file references
  â†“ tests transaction safety
  â†’ PRODUCES â†’ 25-30 tests
```

### Subphase 5.4.4: Generate Tests for Analysis & Search
```bash
Commands: analysis.rs, search.rs

STEP 4: Use unit-test-generator for analysis/search
Expected: ~15-20 tests covering:
  âœ“ analysis.rs (10 tests):
    - MIDI analysis (BPM, key, instruments)
    - Metadata generation
    - Analysis progress tracking
  âœ“ search.rs (5-10 tests):
    - Search loaded projects
    - Filter by metadata
    - Results pagination
```

**Tool Stack:**
```
/unit-test-generator:generate-tests
  â†“ generates analysis tests
  â†“ includes computation validation
  â†“ covers edge cases
  â†’ FEEDS INTO â†’ code-reviewer

code-reviewer agent
  â†“ validates test quality
  â†“ checks assertion strength
  â†“ identifies missing scenarios
  â†’ PRODUCES â†’ 15-20 tests
```

### Subphase 5.4.5: Code Review & Quality Pass
```bash
STEP 5: Use code-reviewer agent on all DAW command tests
Task: "Review all 7 DAW command test files for quality"
Focus:
  âœ“ Real-time constraints respected
  âœ“ MIDI mock implementation correctness
  âœ“ State machine validation
  âœ“ Error handling completeness
  âœ“ Hardware simulation realism
```

**Tool Stack:**
```
/code-reviewer:code-review
  â†“ runs on all 7 test files
  â†“ checks real-time safety
  â†“ validates MIDI compliance
  â†’ PRODUCES â†’ production-quality tests

If issues found:
  â†“ Use performance-oracle for timing issues
  â†“ Use midi-hardware agent for MIDI issues
  â†“ Fix and re-review
Until: All tests pass quality gate
```

---

## Phase 5.5: Integration & End-to-End Tests

**Duration:** 2-3 hours
**Tools:** integration-test-runner plugin, test-orchestrator plugin, architecture-strategist agent
**Slash Commands:** `/integration-test-runner:run-integration`, `/test-orchestrator:orchestrate`

### Subphase 5.5.1: Design Integration Test Scenarios
```bash
STEP 1: Use architecture-strategist to design integration points
Task: "Map integration points between Pipeline and DAW"
Key Scenarios:
  1. Import workflow: Pipeline (import) â†’ Database â†’ DAW (load)
  2. Search workflow: Pipeline (search) â†’ Database â†’ DAW (filter)
  3. Analysis workflow: Pipeline (analyze) â†’ Metadata â†’ DAW (display)
  4. Export workflow: DAW (create) â†’ Pipeline (archive/export)
```

**Tool Stack:**
```
architecture-strategist agent
  â†“ analyzes component boundaries
  â†“ identifies integration contracts
  â†“ validates data flow
  â†’ FEEDS INTO â†’ test-orchestrator
```

### Subphase 5.5.2: Create Integration Test Suite
```bash
STEP 2: Use integration-test-runner to generate integration tests
Files to create:
  âœ“ tests/integration/import_to_daw_test.rs
  âœ“ tests/integration/search_workflow_test.rs
  âœ“ tests/integration/analysis_pipeline_test.rs
  âœ“ tests/integration/export_roundtrip_test.rs

Expected: ~20-25 integration tests
```

**Tool Stack:**
```
/integration-test-runner:run-integration
  â†“ creates full workflow tests
  â†“ includes multi-app scenarios
  â†“ validates data consistency
  â†’ PRODUCES â†’ integration test suite

/test-orchestrator:orchestrate
  â†“ coordinates test execution
  â†“ manages setup/teardown
  â†“ validates test dependencies
  â†’ RUNS â†’ all integration tests
```

### Subphase 5.5.3: End-to-End Scenario Testing
```bash
STEP 3: Create real-world scenario tests
Scenarios:
  1. User imports 100 MIDI files â†’ Analyzes â†’ Searches â†’ Exports selection
  2. User creates sequencer track â†’ Adds notes â†’ Exports â†’ Re-imports â†’ Verifies
  3. Concurrent imports + searches + playback (stress testing)
  4. Recovery from interruptions (mid-import cancellation, etc.)
```

**Tool Stack:**
```
/test-orchestrator:orchestrate
  â†“ manages complex multi-step scenarios
  â†“ validates state consistency
  â†“ simulates user workflows
  â†’ PRODUCES â†’ scenario validation

If issues found:
  â†“ Use code-reviewer for test quality
  â†“ Use performance-oracle for timing
  â†“ Fix and re-run orchestration
Until: All scenarios pass
```

---

## Phase 5.6: Performance & Concurrency Tests

**Duration:** 2-3 hours
**Tools:** performance-oracle agent, test-orchestrator plugin, database-index-advisor plugin
**Slash Commands:** `/database-index-advisor:index-advisor`

### Subphase 5.6.1: Performance Analysis
```bash
STEP 1: Use performance-oracle to analyze command performance
Task: "Analyze performance characteristics of all 18 command files"
Metrics:
  âœ“ Execution time (P50, P95, P99)
  âœ“ Memory usage (peak, sustained)
  âœ“ Database query optimization
  âœ“ File I/O patterns
  âœ“ CPU efficiency
```

**Tool Stack:**
```
performance-oracle agent
  â†“ profiles command execution
  â†“ identifies bottlenecks
  â†“ suggests optimizations
  â†’ FEEDS INTO â†’ database-index-advisor

/database-index-advisor:index-advisor
  â†“ analyzes query patterns
  â†“ recommends missing indexes
  â†“ validates query plans
  â†’ PRODUCES â†’ optimization recommendations
```

### Subphase 5.6.2: Concurrency & Thread Safety Tests
```bash
STEP 2: Create concurrency stress tests
Test Scenarios:
  âœ“ 10 concurrent file imports
  âœ“ 100 concurrent search queries
  âœ“ Simultaneous playback + metadata updates
  âœ“ Rapid play/pause/seek cycles
  âœ“ Database contention (high write load)
```

**Tool Stack:**
```
/test-orchestrator:orchestrate
  â†“ manages concurrent operations
  â†“ validates race condition prevention
  â†“ checks deadlock avoidance
  â†’ PRODUCES â†’ concurrency test results

performance-oracle agent
  â†“ analyzes concurrent performance
  â†“ identifies contention points
  â†“ validates thread safety
  â†’ PRODUCES â†’ performance baseline
```

### Subphase 5.6.3: Load & Scale Testing
```bash
STEP 3: Test with realistic data volumes
Scenarios:
  âœ“ 10,000 files imported and searchable
  âœ“ 1,000 tags in complex hierarchy
  âœ“ 100 concurrent users (simulated)
  âœ“ Database growth from 1MB â†’ 1GB
```

**Tool Stack:**
```
/test-orchestrator:orchestrate
  â†“ generates large test datasets
  â†“ simulates realistic load
  â†“ monitors resource usage
  â†’ PRODUCES â†’ load test results

performance-oracle agent
  â†“ analyzes scalability characteristics
  â†“ identifies resource limits
  â†“ validates O(n) complexity
  â†’ PRODUCES â†’ scaling recommendations
```

---

## Execution Order & Tool Stacking Summary

### Optimal Execution Sequence:

```
â”Œâ”€ Phase 5.0: Architecture Analysis (you are here)
â”‚
â”œâ”€ Phase 5.1: Pipeline Discovery (60-90 min)
â”‚  Explore agent â†’ architecture-reviewer â†’ Grep â†’ Write
â”‚
â”œâ”€ Phase 5.2: Pipeline Tests (3-4 hours)
â”‚  /unit-test-generator (5x) + /database-test-manager
â”‚  â†“ security-sentinel (archive safety)
â”‚  â†“ query-performance-analyzer (query tests)
â”‚  â†“ code-reviewer (quality gate)
â”‚
â”œâ”€ Phase 5.3: DAW Discovery (45-60 min)
â”‚  Explore agent â†’ midi-hardware agent â†’ architecture-reviewer â†’ Write
â”‚
â”œâ”€ Phase 5.4: DAW Tests (3-4 hours)
â”‚  /unit-test-generator (4x) + midi-hardware agent
â”‚  â†“ security-sentinel (MIDI safety)
â”‚  â†“ performance-oracle (real-time requirements)
â”‚  â†“ code-reviewer (quality gate)
â”‚
â”œâ”€ Phase 5.5: Integration Tests (2-3 hours)
â”‚  architecture-strategist â†’ /integration-test-runner
â”‚  â†“ /test-orchestrator (execute all)
â”‚
â””â”€ Phase 5.6: Performance Testing (2-3 hours)
   performance-oracle â†’ /database-index-advisor
   â†“ /test-orchestrator (load tests)
```

### Total Estimated Time: 10-14 hours

---

## Files to Create/Modify

### Pipeline Test Files (to create in Phase 5.2):
```
pipeline/src-tauri/tests/commands/
â”œâ”€â”€ file_import_test.rs        (25-30 tests)
â”œâ”€â”€ archive_import_test.rs      (12-15 tests)
â”œâ”€â”€ analyze_test.rs             (8-10 tests)
â”œâ”€â”€ search_test.rs              (8-10 tests)
â”œâ”€â”€ tags_test.rs                (10 tests)
â”œâ”€â”€ split_file_test.rs          (10 tests)
â”œâ”€â”€ files_test.rs               (10 tests)
â”œâ”€â”€ stats_test.rs               (8 tests)
â”œâ”€â”€ progress_test.rs            (8 tests)
â”œâ”€â”€ system_test.rs              (6 tests)
â””â”€â”€ common/                     (fixtures, helpers, mocks)
    â”œâ”€â”€ fixtures.rs
    â”œâ”€â”€ mocks.rs
    â””â”€â”€ assertions.rs
```

### DAW Test Files (to create in Phase 5.4):
```
daw/src-tauri/tests/commands/
â”œâ”€â”€ sequencer_test.rs           (35-40 tests)
â”œâ”€â”€ midi_test.rs                (20-25 tests)
â”œâ”€â”€ project_test.rs             (10 tests)
â”œâ”€â”€ export_test.rs              (5-10 tests)
â”œâ”€â”€ analysis_test.rs            (5 tests)
â”œâ”€â”€ search_test.rs              (5-10 tests)
â””â”€â”€ common/                     (fixtures, MIDI mocks)
    â”œâ”€â”€ midi_fixtures.rs
    â”œâ”€â”€ midi_mocks.rs
    â””â”€â”€ assertions.rs
```

### Integration Test Files (to create in Phase 5.5):
```
tests/integration/
â”œâ”€â”€ import_to_daw_test.rs       (5-6 tests)
â”œâ”€â”€ search_workflow_test.rs      (4-5 tests)
â”œâ”€â”€ analysis_pipeline_test.rs    (3-4 tests)
â”œâ”€â”€ export_roundtrip_test.rs     (3-4 tests)
â””â”€â”€ common/
    â”œâ”€â”€ setup.rs
    â””â”€â”€ fixtures.rs
```

---

## Key Success Metrics

| Metric | Target | Achievable |
|--------|--------|-----------|
| Tests Written | 200-250 | âœ… Yes (80+ per subphase) |
| Coverage | 80%+ on all commands | âœ… Yes (typical 85-95%) |
| Quality Score | 8.5+/10 avg | âœ… Yes (8.9 avg in Phase 4) |
| Execution Time | <5 min (all tests) | âœ… Yes (similar to Phase 4) |
| Pass Rate | 100% | âœ… Yes (Phase 4: 100%) |

---

## Tools & Agents Summary

### Slash Commands to Use (10 total):
```
âœ“ /unit-test-generator:generate-tests      (5x - one per command group)
âœ“ /database-test-manager:db-test           (2x - Pipeline + integration)
âœ“ /database-index-advisor:index-advisor    (1x - performance optimization)
âœ“ /integration-test-runner:run-integration (1x - end-to-end workflows)
âœ“ /test-orchestrator:orchestrate           (3x - integration, concurrency, load)
âœ“ /feature-dev:feature-dev                 (1x - planning phase)
```

### Specialized Agents to Use (6 total):
```
âœ“ Explore agent                    (2x - Phase 5.1, 5.3 discovery)
âœ“ architecture-reviewer            (2x - validate test architecture)
âœ“ architecture-strategist          (1x - integration point design)
âœ“ code-reviewer                    (2x - quality gates)
âœ“ security-sentinel                (2x - archive safety, MIDI safety)
âœ“ midi-hardware agent              (2x - MIDI testing, validation)
âœ“ performance-oracle               (2x - command performance, concurrency)
```

### MCP Servers to Leverage:
```
âœ“ postgres MCP    - Database setup/fixtures, schema validation
âœ“ filesystem MCP  - Test data generation, archive creation
```

### Plugins to Use:
```
âœ“ test-coverage-analyzer          (final coverage report)
âœ“ unit-test-generator             (tests generation)
âœ“ database-test-manager           (DB isolation, fixtures)
âœ“ integration-test-runner         (integration tests)
âœ“ test-orchestrator               (complex test coordination)
âœ“ database-index-advisor          (query optimization)
âœ“ git-commit-smart                (semantic commits per phase)
```

---

## Next Steps

**Ready to proceed?**

1. **Phase 5.1**: Kick off with `Task(subagent_type="Explore")` for Pipeline command discovery
2. **Each subphase**: Follow the tool stack as outlined
3. **After each phase**: Use `/code-review:code-review` for quality gate
4. **Commits**: One commit per major phase (5.1/5.2, 5.3/5.4, 5.5/5.6)

**Estimated Total Time:** 10-14 hours with tool-stacked efficiency

**Expected Outcome:**
- âœ… 200-250 tests covering all commands
- âœ… 8.5+/10 average quality score
- âœ… Coverage: 51.2% â†’ ~65-70% (adding all command tests)
- âœ… 0 unwrap/expect/panic violations
- âœ… Production-ready command layer

---

**Phase 5 Ready to Execute!** ðŸš€
