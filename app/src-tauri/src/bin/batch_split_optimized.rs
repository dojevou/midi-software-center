/// Batch Split OPTIMIZED - 5-10x faster version
///
/// Optimizations:
/// 1. Parallel batch processing (pipeline multiple batches)
/// 2. Skip duplicate disk writes (check DB first)
/// 3. Larger batch size (default 1000 vs 100)
/// 4. More workers (default 48 vs 24)
/// 5. Increased DB connections
use anyhow::{Context, Result};
use clap::Parser;
use futures::stream::{self, StreamExt};
use indicatif::{ProgressBar, ProgressStyle};
use midi_app::core::splitting::{split_tracks_with_repair, RepairResult};
use sqlx::postgres::PgPoolOptions;
use sqlx::{Pool, Postgres};
use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use tokio::fs;

#[derive(Parser, Debug)]
#[command(name = "batch_split_optimized")]
#[command(about = "Optimized batch split - 5-10x faster")]
struct Args {
    /// Output directory for split files
    #[arg(short, long, default_value = "/home/dojevou/tmp/midi_splits_fast")]
    output_dir: PathBuf,

    /// Number of parallel workers
    #[arg(short, long, default_value_t = 48)]
    workers: usize,

    /// Database connection string
    #[arg(short = 'D', long, env = "DATABASE_URL")]
    database_url: String,

    /// Batch size for database queries
    #[arg(short = 'b', long, default_value_t = 1000)]
    batch_size: i64,

    /// Number of parallel batches to process
    #[arg(short = 'p', long, default_value_t = 4)]
    parallel_batches: usize,

    /// Test mode - only process first N files
    #[arg(short = 't', long)]
    test_limit: Option<i64>,
}

#[derive(Debug, Clone)]
#[allow(dead_code)]
struct MultiTrackFile {
    id: i64,
    filepath: String,
    filename: String,
    num_tracks: i16,
}

#[derive(Debug)]
struct Stats {
    files_processed: AtomicU64,
    tracks_created: AtomicU64,
    files_skipped: AtomicU64,
    errors: AtomicU64,
    duplicates_avoided: AtomicU64,
    files_repaired: AtomicU64,
    files_corrupt: AtomicU64,
}

impl Stats {
    fn new() -> Self {
        Self {
            files_processed: AtomicU64::new(0),
            tracks_created: AtomicU64::new(0),
            files_skipped: AtomicU64::new(0),
            errors: AtomicU64::new(0),
            duplicates_avoided: AtomicU64::new(0),
            files_repaired: AtomicU64::new(0),
            files_corrupt: AtomicU64::new(0),
        }
    }

    fn inc_processed(&self) {
        self.files_processed.fetch_add(1, Ordering::Relaxed);
    }

    fn inc_tracks(&self, count: u64) {
        self.tracks_created.fetch_add(count, Ordering::Relaxed);
    }

    #[allow(dead_code)]
    fn inc_skipped(&self) {
        self.files_skipped.fetch_add(1, Ordering::Relaxed);
    }

    fn inc_errors(&self) {
        self.errors.fetch_add(1, Ordering::Relaxed);
    }

    fn inc_duplicates(&self) {
        self.duplicates_avoided.fetch_add(1, Ordering::Relaxed);
    }

    fn inc_repaired(&self) {
        self.files_repaired.fetch_add(1, Ordering::Relaxed);
    }

    #[allow(dead_code)]
    fn inc_corrupt(&self) {
        self.files_corrupt.fetch_add(1, Ordering::Relaxed);
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    println!("ğŸš€ BATCH MIDI SPLIT TOOL - OPTIMIZED");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Output directory: {:?}", args.output_dir);
    println!("Workers: {}", args.workers);
    println!("Batch size: {}", args.batch_size);
    println!("Parallel batches: {}", args.parallel_batches);
    println!();

    // Create output directory
    if !args.output_dir.exists() {
        println!("ğŸ“ Creating output directory...");
        fs::create_dir_all(&args.output_dir).await?;
    }

    // Connect to database with more connections
    println!("ğŸ”Œ Connecting to database...");
    let pool = PgPoolOptions::new()
        .max_connections(args.workers as u32 + 20)  // Increased from +5
        .connect(&args.database_url)
        .await?;
    println!("âœ… Database connected");

    // Count multi-track files
    let count_query = if let Some(limit) = args.test_limit {
        format!(
            "SELECT COUNT(*) as count FROM (SELECT id FROM files WHERE num_tracks > 1 LIMIT {}) as limited",
            limit
        )
    } else {
        "SELECT COUNT(*) as count FROM files WHERE num_tracks > 1".to_string()
    };

    let count: i64 = sqlx::query_scalar(&count_query).fetch_one(&pool).await?;

    println!("ğŸ“Š Found {} multi-track files to split", count);
    println!();

    if count == 0 {
        println!("âœ… No files to split!");
        return Ok(());
    }

    // Create progress bar
    let progress = Arc::new(ProgressBar::new(count as u64));
    progress.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{bar:40.cyan/blue}] {pos}/{len} ({percent}%) - {msg} [{elapsed_precise}]")?
            .progress_chars("â–ˆâ–“â–’â–‘ "),
    );

    // Initialize stats
    let stats = Arc::new(Stats::new());

    // Calculate total batches
    let total_batches = (count + args.batch_size - 1) / args.batch_size;

    // OPTIMIZATION: Process batches in parallel using stream
    let batch_stream = stream::iter(0..total_batches)
        .map(|batch_num| {
            let pool = pool.clone();
            let output_dir = args.output_dir.clone();
            let stats = stats.clone();
            let progress = progress.clone();
            let batch_size = args.batch_size;
            let test_limit = args.test_limit;
            let workers = args.workers;

            async move {
                let offset = batch_num * batch_size;

                // Fetch batch
                let files = match fetch_batch(&pool, offset, batch_size, test_limit).await {
                    Ok(f) => f,
                    Err(e) => {
                        eprintln!("Error fetching batch {}: {}", batch_num, e);
                        return;
                    },
                };

                if files.is_empty() {
                    return;
                }

                // Process files in batch with concurrency limit
                stream::iter(files)
                    .map(|file| {
                        let pool = pool.clone();
                        let output_dir = output_dir.clone();
                        let stats = stats.clone();
                        let progress = progress.clone();

                        async move {
                            match process_file_optimized(&pool, &output_dir, file).await {
                                Ok((track_count, duplicates, was_repaired)) => {
                                    stats.inc_processed();
                                    stats.inc_tracks(track_count as u64);
                                    if duplicates > 0 {
                                        stats.inc_duplicates();
                                    }
                                    if was_repaired {
                                        stats.inc_repaired();
                                    }
                                    progress.inc(1);
                                },
                                Err(e) => {
                                    stats.inc_errors();
                                    progress.inc(1);
                                    eprintln!("Error: {}", e);
                                },
                            }
                        }
                    })
                    .buffer_unordered(workers)
                    .collect::<Vec<_>>()
                    .await;
            }
        })
        .buffer_unordered(args.parallel_batches);

    // Execute all batches
    batch_stream.collect::<Vec<_>>().await;

    progress.finish_with_message("Complete!");
    println!();
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ… SPLIT COMPLETE (OPTIMIZED WITH AUTO-REPAIR)");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!(
        "Files processed:    {}",
        stats.files_processed.load(Ordering::Relaxed)
    );
    println!(
        "Tracks created:     {}",
        stats.tracks_created.load(Ordering::Relaxed)
    );
    println!(
        "ğŸ”§ Files repaired:  {}",
        stats.files_repaired.load(Ordering::Relaxed)
    );
    println!(
        "âŒ Files corrupt:   {}",
        stats.files_corrupt.load(Ordering::Relaxed)
    );
    println!(
        "Duplicates avoided: {}",
        stats.duplicates_avoided.load(Ordering::Relaxed)
    );
    println!(
        "Files skipped:      {}",
        stats.files_skipped.load(Ordering::Relaxed)
    );
    println!(
        "Errors:             {}",
        stats.errors.load(Ordering::Relaxed)
    );
    println!();

    Ok(())
}

async fn fetch_batch(
    pool: &Pool<Postgres>,
    offset: i64,
    limit: i64,
    _test_limit: Option<i64>,
) -> Result<Vec<MultiTrackFile>> {
    let files = sqlx::query_as!(
        MultiTrackFile,
        r#"
        SELECT id, filepath, filename, num_tracks
        FROM files
        WHERE num_tracks > 1
        ORDER BY id
        LIMIT $1 OFFSET $2
        "#,
        limit,
        offset
    )
    .fetch_all(pool)
    .await?;

    Ok(files)
}

/// Result of processing a file: (track_count, duplicate_count, was_repaired)
type ProcessResult = (usize, usize, bool);

/// OPTIMIZED: Check database FIRST, only write if not duplicate
/// NOW WITH AUTO-REPAIR: Automatically fixes corrupted MIDI files
async fn process_file_optimized(
    pool: &Pool<Postgres>,
    output_dir: &Path,
    file: MultiTrackFile,
) -> Result<ProcessResult> {
    // Read MIDI file
    let midi_bytes = fs::read(&file.filepath)
        .await
        .context(format!("Failed to read file: {}", file.filepath))?;

    // Split tracks with automatic repair
    let (split_tracks, repair_result) = split_tracks_with_repair(&midi_bytes)
        .map_err(|e| anyhow::anyhow!("Failed to split {}: {}", file.filename, e))?;

    // Track repair status and log
    let was_repaired = match &repair_result {
        RepairResult::Valid => {
            // File was valid, no repair needed
            false
        },
        RepairResult::Repaired { fix_description, .. } => {
            println!("ğŸ”§ REPAIRED: {} - {}", file.filename, fix_description);
            true
        },
        RepairResult::Corrupt { reason } => {
            eprintln!("âŒ CORRUPT: {} - {}", file.filename, reason);
            false
        },
    };

    let track_count = split_tracks.len();
    let mut duplicate_count = 0;

    // Save each split track
    for split_track in split_tracks.iter() {
        // Generate filename
        let base_name = Path::new(&file.filename)
            .file_stem()
            .and_then(|s| s.to_str())
            .unwrap_or("track");

        let track_name = split_track
            .track_name
            .as_ref()
            .map(|n| sanitize_filename(n))
            .unwrap_or_default();

        let output_filename = if track_name.is_empty() {
            format!("{}_{:02}.mid", base_name, split_track.track_number + 1)
        } else {
            format!(
                "{}_{:02}_{}.mid",
                base_name,
                split_track.track_number + 1,
                track_name
            )
        };

        // Include parent file ID in path to avoid collisions
        let output_path = output_dir.join(format!("{}_{}", file.id, &output_filename));

        // Calculate hash BEFORE writing
        let hash = blake3::hash(&split_track.midi_bytes);

        // OPTIMIZATION: Try to insert FIRST, only write file if successful
        let insert_result: Result<Option<(i64,)>, sqlx::Error> = sqlx::query_as(
            r#"
            INSERT INTO files (
                filename, filepath, original_filename,
                content_hash, file_size_bytes, num_tracks
            ) VALUES ($1, $2, $3, $4, $5, 1)
            ON CONFLICT (content_hash) DO NOTHING
            RETURNING id
            "#,
        )
        .bind(&output_filename)
        .bind(output_path.to_str().unwrap_or(""))
        .bind(&output_filename)
        .bind(hash.as_bytes())
        .bind(split_track.midi_bytes.len() as i64)
        .fetch_optional(pool)
        .await;

        match insert_result {
            Ok(Some((split_file_id,))) => {
                // NEW FILE: Write to disk
                fs::write(&output_path, &split_track.midi_bytes).await?;

                // Insert into track_splits
                let track_name = split_track.track_name.clone();
                let instrument = split_track.instrument.clone();
                let note_count = split_track.note_count as i32;

                let _ = sqlx::query!(
                    r#"
                    INSERT INTO track_splits (
                        parent_file_id, split_file_id, track_number,
                        track_name, instrument, note_count
                    ) VALUES ($1, $2, $3, $4, $5, $6)
                    ON CONFLICT (parent_file_id, split_file_id) DO NOTHING
                    "#,
                    file.id,
                    split_file_id,
                    split_track.track_number as i32,
                    track_name,
                    instrument,
                    note_count
                )
                .execute(pool)
                .await;
            },
            Ok(None) => {
                // DUPLICATE: Skip disk write entirely!
                duplicate_count += 1;
            },
            Err(e) => {
                eprintln!("âš ï¸  Database error for {}: {}", output_filename, e);
            },
        }
    }

    Ok((track_count, duplicate_count, was_repaired))
}

fn sanitize_filename(name: &str) -> String {
    name.chars()
        .map(|c| match c {
            '/' | '\\' | ':' | '*' | '?' | '"' | '<' | '>' | '|' => '_',
            c if c.is_control() => '_',
            c => c,
        })
        .collect::<String>()
        .trim()
        .to_string()
}
